{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mipademiao/IDL_PJ_21Spring/blob/master/run_mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W854WjpfLnWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c2da43-918a-4ee9-e3a9-ce40001e63d8"
      },
      "source": [
        "#!pip install pytorch-transformers\n",
        "#use latest version of transformers instead of pytorch-transformers\n",
        "!pip install transformers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GZkZxllEtp",
        "outputId": "ec5570e8-99da-446e-f4d9-01729eb9944c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCWdBxlDOPOn",
        "outputId": "18846544-e1c1-497f-c49e-9c9b832c21ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 20.8MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 58.0MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIsBJ6h_Ppf5",
        "outputId": "002d237d-833a-4e3e-c299-0359013ca7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72060, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 72060 (delta 24), reused 41 (delta 11), pack-reused 71972\u001b[K\n",
            "Receiving objects: 100% (72060/72060), 54.81 MiB | 29.82 MiB/s, done.\n",
            "Resolving deltas: 100% (51156/51156), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90-KDddQSmm8",
        "outputId": "1a90ed39-fc7e-4fe5-ae41-49c37634c30b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cd transformers\n",
        "!pip install transformers/."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2213074 sha256=26f01bae889518a1e8c75d17155a645ac2c8bcfab0e7eace5e088084bb528e07\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wmth0ax6/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.5.1\n",
            "    Uninstalling transformers-4.5.1:\n",
            "      Successfully uninstalled transformers-4.5.1\n",
            "Successfully installed transformers-4.6.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "277Yk8WfSq9d",
        "outputId": "3757489a-4e38-46b3-d509-fa468a2b1beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python /content/drive/My\\ Drive/Colab\\ Notebooks/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "    --model_name_or_path distilbert-base-multilingual-cased \\\n",
        "    --train_file /content/drive/My\\ Drive/Colab\\ Notebooks/train_full_filtered.txt \\\n",
        "    --validation_file /content/drive/My\\ Drive/Colab\\ Notebooks/train_full_filtered.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/drive/My\\ Drive/Colab\\ Notebooks/tmp/test-mlm"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-08 00:10:38.065223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/08/2021 00:10:39 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/08/2021 00:10:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/My Drive/Colab Notebooks/tmp/test-mlm, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May08_00-10-39_90251d8fca81, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/My Drive/Colab Notebooks/tmp/test-mlm, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
            "05/08/2021 00:10:39 - WARNING - datasets.builder -   Using custom data configuration default-56d1a1ee21723fe5\n",
            "05/08/2021 00:10:39 - WARNING - datasets.builder -   Reusing dataset text (/root/.cache/huggingface/datasets/text/default-56d1a1ee21723fe5/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "[INFO|file_utils.py:1531] 2021-05-08 00:10:39,990 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxtnz3_iw\n",
            "Downloading: 100% 466/466 [00:00<00:00, 741kB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 00:10:40,008 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|file_utils.py:1538] 2021-05-08 00:10:40,008 >> creating metadata file for /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|configuration_utils.py:517] 2021-05-08 00:10:40,008 >> loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|configuration_utils.py:553] 2021-05-08 00:10:40,009 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:517] 2021-05-08 00:10:40,025 >> loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|configuration_utils.py:553] 2021-05-08 00:10:40,025 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1531] 2021-05-08 00:10:40,047 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwe9j3qwp\n",
            "Downloading: 100% 996k/996k [00:00<00:00, 21.1MB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 00:10:40,121 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:1538] 2021-05-08 00:10:40,121 >> creating metadata file for /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:1531] 2021-05-08 00:10:40,141 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjdt8rzhf\n",
            "Downloading: 100% 1.96M/1.96M [00:00<00:00, 38.1MB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 00:10:40,227 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|file_utils.py:1538] 2021-05-08 00:10:40,227 >> creating metadata file for /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|file_utils.py:1531] 2021-05-08 00:10:40,272 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6ndoth5w\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 47.7kB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 00:10:40,292 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1538] 2021-05-08 00:10:40,292 >> creating metadata file for /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 00:10:40,292 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 00:10:40,292 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 00:10:40,292 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 00:10:40,293 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 00:10:40,293 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1531] 2021-05-08 00:10:40,415 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpffw8kwl0\n",
            "Downloading: 100% 542M/542M [00:13<00:00, 40.3MB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 00:10:54,003 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
            "[INFO|file_utils.py:1538] 2021-05-08 00:10:54,003 >> creating metadata file for /root/.cache/huggingface/transformers/7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
            "[INFO|modeling_utils.py:1149] 2021-05-08 00:10:54,003 >> loading weights file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
            "[INFO|modeling_utils.py:1331] 2021-05-08 00:10:56,204 >> All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1340] 2021-05-08 00:10:56,204 >> All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
            "100% 1/1 [00:00<00:00,  8.96ba/s]\n",
            "100% 1/1 [00:00<00:00, 22.80ba/s]\n",
            "100% 1/1 [00:00<00:00, 16.63ba/s]\n",
            "100% 1/1 [00:00<00:00, 17.97ba/s]\n",
            "[INFO|trainer.py:515] 2021-05-08 00:11:07,683 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1144] 2021-05-08 00:11:07,691 >> ***** Running training *****\n",
            "[INFO|trainer.py:1145] 2021-05-08 00:11:07,691 >>   Num examples = 24\n",
            "[INFO|trainer.py:1146] 2021-05-08 00:11:07,691 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1147] 2021-05-08 00:11:07,691 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1148] 2021-05-08 00:11:07,691 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1149] 2021-05-08 00:11:07,691 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1150] 2021-05-08 00:11:07,691 >>   Total optimization steps = 9\n",
            "100% 9/9 [00:10<00:00,  1.14s/it][INFO|trainer.py:1340] 2021-05-08 00:11:18,029 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 10.3379, 'train_samples_per_second': 0.871, 'epoch': 3.0}\n",
            "100% 9/9 [00:10<00:00,  1.15s/it]\n",
            "[INFO|trainer.py:1867] 2021-05-08 00:11:18,185 >> Saving model checkpoint to /content/drive/My Drive/Colab Notebooks/tmp/test-mlm\n",
            "[INFO|configuration_utils.py:351] 2021-05-08 00:11:18,205 >> Configuration saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm/config.json\n",
            "[INFO|modeling_utils.py:883] 2021-05-08 00:11:20,346 >> Model weights saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-08 00:11:20,350 >> tokenizer config file saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-08 00:11:20,358 >> Special tokens file saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:907] 2021-05-08 00:11:20,582 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   init_mem_cpu_alloc_delta   =     1817MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   init_mem_gpu_alloc_delta   =      516MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   train_mem_cpu_alloc_delta  =       10MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   train_mem_gpu_alloc_delta  =     1583MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   train_mem_gpu_peaked_delta =    10577MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   train_runtime              = 0:00:10.33\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   train_samples              =         24\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:20,583 >>   train_samples_per_second   =      0.871\n",
            "05/08/2021 00:11:20 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:515] 2021-05-08 00:11:20,716 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2089] 2021-05-08 00:11:20,719 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2091] 2021-05-08 00:11:20,719 >>   Num examples = 24\n",
            "[INFO|trainer.py:2094] 2021-05-08 00:11:20,719 >>   Batch size = 8\n",
            "100% 3/3 [00:01<00:00,  2.53it/s]\n",
            "[INFO|trainer_pt_utils.py:907] 2021-05-08 00:11:22,053 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,053 >>   epoch                     =        3.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_loss                 =      3.069\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_mem_gpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_mem_gpu_peaked_delta =     3748MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_runtime              = 0:00:01.22\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_samples              =         24\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   eval_samples_per_second   =     19.638\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 00:11:22,054 >>   perplexity                =     21.521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrG85jUeTNkt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}