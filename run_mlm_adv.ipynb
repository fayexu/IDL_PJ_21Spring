{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mipademiao/IDL_PJ_21Spring/blob/master/run_mlm_adv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W854WjpfLnWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0801d1-c362-411c-a5af-06b0fb462117"
      },
      "source": [
        "#!pip install pytorch-transformers\n",
        "#use latest version of transformers instead of pytorch-transformers\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\r\u001b[K     |▏                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 32.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 23.6MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 26.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 25.3MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 27.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 18.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 19.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 18.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 18.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 18.6MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 18.6MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 18.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 18.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 18.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 18.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 18.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 18.6MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 18.6MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 296kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 348kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 368kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 399kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 409kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 419kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 430kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 440kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 460kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 471kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 481kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 491kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 501kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 532kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 542kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 552kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 563kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 593kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 604kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 614kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 624kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 634kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 645kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 655kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 665kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 675kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 686kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 696kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 706kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 716kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 727kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 737kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 747kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 757kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 788kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 798kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 808kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 819kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 829kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 839kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 849kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 860kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 870kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 880kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 890kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 901kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 911kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 921kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 931kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 942kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 952kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 962kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 972kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 983kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 993kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GZkZxllEtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b3857a-a1e8-4f61-f34e-61af8afe249c"
      },
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCWdBxlDOPOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b400a484-1c94-4b1c-d3d0-cedb6f5bfa03"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIsBJ6h_Ppf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55493f63-fef0-4283-fcc2-3e1cf2d0db44"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72060, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 72060 (delta 23), reused 41 (delta 11), pack-reused 71973\u001b[K\n",
            "Receiving objects: 100% (72060/72060), 54.82 MiB | 29.79 MiB/s, done.\n",
            "Resolving deltas: 100% (51148/51148), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90-KDddQSmm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36275ca3-6f7e-4686-fc0f-357b9b4c2b4e"
      },
      "source": [
        "!pip install transformers/."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2213074 sha256=ca774af529ea0ce049f775e0bcb4b8ab77f8768ae3edc9e6a97a5ca75f9cc7f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wkw7k2py/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.5.1\n",
            "    Uninstalling transformers-4.5.1:\n",
            "      Successfully uninstalled transformers-4.5.1\n",
            "Successfully installed transformers-4.6.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "277Yk8WfSq9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3f11ff-ee13-42ef-9c1b-df19b51734a6"
      },
      "source": [
        "!python /content/drive/My\\ Drive/Colab\\ Notebooks/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "    --model_name_or_path distilbert-base-multilingual-cased \\\n",
        "    --train_file /content/drive/My\\ Drive/Colab\\ Notebooks/out.txt \\\n",
        "    --validation_file /content/drive/My\\ Drive/Colab\\ Notebooks/out.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/drive/My\\ Drive/Colab\\ Notebooks/tmp/test-mlm_adv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-08 13:23:00.948092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/08/2021 13:23:02 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/08/2021 13:23:02 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/May08_13-23-02_1be195646ce1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\n",
            "05/08/2021 13:23:02 - WARNING - datasets.builder -   Using custom data configuration default-8c2bbc591a21a48b\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-8c2bbc591a21a48b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-8c2bbc591a21a48b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1531] 2021-05-08 13:23:02,982 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpx9owru60\n",
            "Downloading: 100% 466/466 [00:00<00:00, 689kB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 13:23:03,088 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|file_utils.py:1538] 2021-05-08 13:23:03,089 >> creating metadata file for /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|configuration_utils.py:517] 2021-05-08 13:23:03,089 >> loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|configuration_utils.py:553] 2021-05-08 13:23:03,089 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:517] 2021-05-08 13:23:03,106 >> loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
            "[INFO|configuration_utils.py:553] 2021-05-08 13:23:03,106 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1531] 2021-05-08 13:23:03,123 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjbjcezff\n",
            "Downloading: 100% 996k/996k [00:00<00:00, 24.4MB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 13:23:03,190 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:1538] 2021-05-08 13:23:03,190 >> creating metadata file for /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:1531] 2021-05-08 13:23:03,207 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptczcrlzl\n",
            "Downloading: 100% 1.96M/1.96M [00:00<00:00, 38.4MB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 13:23:03,290 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|file_utils.py:1538] 2021-05-08 13:23:03,290 >> creating metadata file for /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|file_utils.py:1531] 2021-05-08 13:23:03,339 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd3fu3242\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 47.5kB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 13:23:03,356 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1538] 2021-05-08 13:23:03,356 >> creating metadata file for /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 13:23:03,356 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 13:23:03,356 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 13:23:03,356 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 13:23:03,356 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-05-08 13:23:03,356 >> loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1531] 2021-05-08 13:23:03,473 >> https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbbwduuru\n",
            "Downloading: 100% 542M/542M [00:10<00:00, 53.0MB/s]\n",
            "[INFO|file_utils.py:1535] 2021-05-08 13:23:13,934 >> storing https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
            "[INFO|file_utils.py:1538] 2021-05-08 13:23:13,934 >> creating metadata file for /root/.cache/huggingface/transformers/7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
            "[INFO|modeling_utils.py:1149] 2021-05-08 13:23:13,935 >> loading weights file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
            "[INFO|modeling_utils.py:1331] 2021-05-08 13:23:16,089 >> All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1340] 2021-05-08 13:23:16,089 >> All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
            "100% 2/2 [00:00<00:00, 17.66ba/s]\n",
            "100% 2/2 [00:00<00:00, 32.70ba/s]\n",
            "100% 2/2 [00:00<00:00, 12.24ba/s]\n",
            "100% 2/2 [00:00<00:00, 12.76ba/s]\n",
            "[INFO|trainer.py:515] 2021-05-08 13:23:27,737 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1144] 2021-05-08 13:23:27,747 >> ***** Running training *****\n",
            "[INFO|trainer.py:1145] 2021-05-08 13:23:27,747 >>   Num examples = 60\n",
            "[INFO|trainer.py:1146] 2021-05-08 13:23:27,747 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1147] 2021-05-08 13:23:27,747 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1148] 2021-05-08 13:23:27,747 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1149] 2021-05-08 13:23:27,747 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1150] 2021-05-08 13:23:27,747 >>   Total optimization steps = 24\n",
            "100% 24/24 [00:26<00:00,  1.02it/s][INFO|trainer.py:1340] 2021-05-08 13:23:53,984 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 26.2372, 'train_samples_per_second': 0.915, 'epoch': 3.0}\n",
            "100% 24/24 [00:26<00:00,  1.09s/it]\n",
            "[INFO|trainer.py:1867] 2021-05-08 13:23:54,140 >> Saving model checkpoint to /content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv\n",
            "[INFO|configuration_utils.py:351] 2021-05-08 13:23:54,145 >> Configuration saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv/config.json\n",
            "[INFO|modeling_utils.py:883] 2021-05-08 13:23:56,297 >> Model weights saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-05-08 13:23:56,303 >> tokenizer config file saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-05-08 13:23:56,308 >> Special tokens file saved in /content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:907] 2021-05-08 13:23:56,541 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,541 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,541 >>   init_mem_cpu_alloc_delta   =     1812MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   init_mem_gpu_alloc_delta   =      516MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   train_mem_cpu_alloc_delta  =       11MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   train_mem_gpu_alloc_delta  =     1583MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   train_mem_gpu_peaked_delta =    10577MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   train_runtime              = 0:00:26.23\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   train_samples              =         60\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:56,542 >>   train_samples_per_second   =      0.915\n",
            "05/08/2021 13:23:56 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:515] 2021-05-08 13:23:56,662 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2089] 2021-05-08 13:23:56,665 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2091] 2021-05-08 13:23:56,665 >>   Num examples = 60\n",
            "[INFO|trainer.py:2094] 2021-05-08 13:23:56,665 >>   Batch size = 8\n",
            "100% 8/8 [00:03<00:00,  2.52it/s]\n",
            "[INFO|trainer_pt_utils.py:907] 2021-05-08 13:23:59,994 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,994 >>   epoch                     =        3.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,994 >>   eval_loss                 =     0.6286\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   eval_mem_gpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   eval_mem_gpu_peaked_delta =     3748MB\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   eval_runtime              = 0:00:03.20\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   eval_samples              =         60\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   eval_samples_per_second   =       18.7\n",
            "[INFO|trainer_pt_utils.py:912] 2021-05-08 13:23:59,995 >>   perplexity                =      1.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrG85jUeTNkt",
        "outputId": "820ac40d-3036-4d30-add8-f3e9bb4ee3d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertForMaskedLM, DistilBertConfig\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/tmp/test-mlm_adv/pytorch_model.bin\"\n",
        "configuration = DistilBertConfig()\n",
        "bert_model = DistilBertModel(configuration)\n",
        "model_state_dict = torch.load(file_path)\n",
        "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-multilingual-cased\", state_dict=model_state_dict)\n",
        "model.eval()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForMaskedLM(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (vocab_projector): Linear(in_features=768, out_features=119547, bias=True)\n",
              "  (mlm_loss_fct): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqbBEm-IKMrA"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-multilingual-cased\") \n",
        "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    \n",
        ")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYSgVEWeKAln"
      },
      "source": [
        "def predict_dist(line):\n",
        "  res = []\n",
        "  prediction = fill_mask(line)\n",
        "  for p in prediction:\n",
        "    res.append((p['token_str'].lower(), p['score']*100))\n",
        "  return res"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuM905XIO8qA",
        "outputId": "153bb740-cdfd-4d8e-af98-a61ff32eb5c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t = predict_dist(\"Someone winds up [MASK] right arm and knocks the fighter down with a haymaker.\")\n",
        "print(t)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('his', 54.17901277542114), ('the', 16.876454651355743), ('its', 10.714925080537796), ('their', 3.3157221972942352), ('her', 3.0493007972836494)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AilYq-1wO9JY"
      },
      "source": [
        "\n",
        "scores = []"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ti5zOCdPPzn"
      },
      "source": [
        "def get_bias_ratio(top_k):\n",
        "  male_token = [\"he\", \"his\", \"him\", \"himself\", \"cowboy\", \"cowboys\", \"male\", \"males\", \"father\", \"grandfather\", \"dad\", \"daddy\" ,\"son\", \"grandson\", \"man\", \"men\", \"waiter\", \"waiters\", \"gentleman\", \"gentlemen\", \"boy\", \"boys\", \"mr\", \"actor\", \"actors\", \"husband\", \"boyfriend\", \"brother\", \"brothers\", \"uncle\", \"uncles\", \"guy\", \"guys\"]\n",
        "  female_token = [\"she\", \"her\", \"hers\", \"herself\", \"witch\", \"witches\", \"female\", \"females\", \"mother\", \"grandmother\", \"mom\", \"mum\", \"mommy\", \"daughter\", \"granddaughter\", \"woman\", \"women\", \"waitress\", \"waitresses\", \"lady\", \"ladies\", \"girl\", \"girls\", \"miss\", \"mrs\", \"actress\", \"actresses\", \"wife\", \"girlfriend\", \"sister\", \"sisters\", \"aunt\", \"aunts\", \"beauty\"] \n",
        "  max_male_token = 'None'\n",
        "  max_female_token = 'None'\n",
        "  max_male_score = 0\n",
        "  max_female_score = 0\n",
        "  for token, score in top_k:\n",
        "    if token in male_token:\n",
        "      max_male_token = token\n",
        "      max_male_score = score\n",
        "      break\n",
        "  for token, score in top_k:\n",
        "    if token in female_token:\n",
        "      max_female_token = token\n",
        "      max_female_score = score\n",
        "      break\n",
        "  ratio = 0\n",
        "  #if max_male_score > max_female_score:\n",
        "  if max_male_token == 'None' and max_female_token == 'None':\n",
        "    #print(\"no solution\\n\")\n",
        "    return [0, 0];\n",
        "  if max_male_score > max_female_score and max_female_score > 1:\n",
        "    ratio = round(max_male_score / (max_female_score + max_male_score), 3)\n",
        "    #print('Male dominates: bias score = {} with {} = {} and {} = {}\\n'.format(ratio, max_male_token, max_male_score, max_female_token, max_female_score))\n",
        "    #print(ratio)\n",
        "    scores.append(ratio)\n",
        "    #very bias\n",
        "    if ratio > 0.8: \n",
        "      print(ratio)\n",
        "    return [ratio, 0]\n",
        "  elif max_male_score < max_female_score and max_male_score > 1:\n",
        "    ratio = round(max_female_score / (max_female_score + max_male_score), 3)\n",
        "    #print('Female dominates: bias score = {} with {} = {} and {} = {}\\n'.format(1-ratio, max_female_token, max_female_score, max_male_token, max_male_score))\n",
        "    #print(1-ratio)\n",
        "    scores.append(1-ratio)\n",
        "    return [ratio, 1]\n",
        "  return [0,0]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUeXWQSQPVhD"
      },
      "source": [
        "def getAverageRatio(input):\n",
        "  #out = open(/content/\"distribution.txt\", \"w\")\n",
        "  with open(input, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    male_count = 0\n",
        "    max_male_ratio = 0\n",
        "    sum_male_ratio = 0\n",
        "    female_count = 0\n",
        "    no_bias_count = 0\n",
        "    max_female_ratio = 0\n",
        "    sum_female_ratio = 0\n",
        "    for line in lines:\n",
        "      #print(line[0:-1])\n",
        "      k = 10\n",
        "      top_prediction = predict_dist(line)\n",
        "      ratio = get_bias_ratio(top_prediction)\n",
        "      if ratio[0] != 0:\n",
        "        if ratio[1] == 0:\n",
        "          sum_male_ratio += ratio[0]\n",
        "          male_count += 1\n",
        "          if ratio[0] > max_male_ratio:\n",
        "            max_male_ratio = ratio[0]\n",
        "            max_male = line[0:-1]\n",
        "        else:\n",
        "          sum_female_ratio += ratio[0]\n",
        "          female_count += 1\n",
        "          if ratio[0] > max_female_ratio:\n",
        "            max_female_ratio = ratio[0]\n",
        "            max_female = line[0:-1]\n",
        "    \n",
        "    print(\"Total sentence: \", len(lines))\n",
        "    print(\"Male count: \", male_count)\n",
        "    print(\"Female count: \", female_count)\n",
        "    print(\"Most biased to male: \", max_male, \"  Ratio = \", max_male_ratio)\n",
        "    print(\"Most biased to female: \", max_female, \"  Ratio = \", 1-max_female_ratio)\n",
        "    return [sum_male_ratio/male_count, sum_female_ratio/female_count]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEwusmC0PZ31",
        "outputId": "6085169b-6306-4cb2-fcbc-29b6ed5b0364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "male_dom, female_dom = getAverageRatio(\"/content/drive/My Drive/Colab Notebooks/train_full_filtered.txt\")\n",
        "print(\"When male dominates: \", male_dom)\n",
        "print(\"When female dominates: \", 1-female_dom)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.819\n",
            "0.854\n",
            "0.838\n",
            "0.832\n",
            "0.947\n",
            "0.819\n",
            "0.833\n",
            "0.865\n",
            "0.858\n",
            "0.841\n",
            "0.841\n",
            "0.899\n",
            "0.933\n",
            "0.898\n",
            "0.92\n",
            "0.896\n",
            "0.811\n",
            "0.866\n",
            "0.943\n",
            "0.907\n",
            "0.824\n",
            "0.959\n",
            "0.854\n",
            "0.804\n",
            "0.953\n",
            "0.953\n",
            "0.874\n",
            "0.914\n",
            "0.896\n",
            "0.952\n",
            "0.905\n",
            "0.86\n",
            "0.908\n",
            "0.957\n",
            "0.935\n",
            "0.915\n",
            "0.832\n",
            "0.849\n",
            "0.821\n",
            "0.912\n",
            "0.894\n",
            "0.85\n",
            "0.839\n",
            "0.899\n",
            "0.813\n",
            "0.81\n",
            "0.86\n",
            "0.889\n",
            "0.922\n",
            "0.908\n",
            "0.832\n",
            "0.848\n",
            "0.951\n",
            "0.861\n",
            "0.874\n",
            "0.857\n",
            "0.91\n",
            "0.877\n",
            "0.86\n",
            "0.85\n",
            "0.817\n",
            "0.842\n",
            "Total sentence:  641\n",
            "Male count:  133\n",
            "Female count:  14\n",
            "Most biased to male:  The Nazi officer touches the side of [MASK] own face .   Ratio =  0.959\n",
            "Most biased to female:  The baby laughs as [MASK] swings back and forth .   Ratio =  0.29300000000000004\n",
            "When male dominates:  0.7686165413533831\n",
            "When female dominates:  0.39621428571428574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzAw0d0tPcSJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}